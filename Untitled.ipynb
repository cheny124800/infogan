{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blub\n",
      "Namespace(Diters=5, batch_size=64, dataroot='./data/mnist', dataset='chairs', experiment=None, file='/home/yz6/.local/share/jupyter/runtime/kernel-501adab4-9060-412f-b239-c81f50efdb8d.json', gpu_device=0, image_size=64, lr=5e-05, max_iter=1000, nc=1, netD='', netG='', nz=10, workers=4)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "import util\n",
    "import numpy as np\n",
    "\n",
    "import base_module\n",
    "from mmd import mix_rbf_mmd2\n",
    "\n",
    "\n",
    "# NetG is a decoder\n",
    "# input: batch_size * nz * 1 * 1\n",
    "# output: batch_size * nc * image_size * image_size\n",
    "class NetG(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super(NetG, self).__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.decoder(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "# NetD is an encoder + decoder\n",
    "# input: batch_size * nc * image_size * image_size\n",
    "# f_enc_X: batch_size * k * 1 * 1\n",
    "# f_dec_X: batch_size * nc * image_size * image_size\n",
    "class NetD(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(NetD, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input):\n",
    "        f_enc_X = self.encoder(input)\n",
    "        f_dec_X = self.decoder(f_enc_X)\n",
    "\n",
    "        f_enc_X = f_enc_X.view(input.size(0), -1)\n",
    "        f_dec_X = f_dec_X.view(input.size(0), -1)\n",
    "        return f_enc_X, f_dec_X\n",
    "\n",
    "\n",
    "class ONE_SIDED(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ONE_SIDED, self).__init__()\n",
    "\n",
    "        main = nn.ReLU()\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(-input)\n",
    "        output = -output.mean()\n",
    "        return output\n",
    "\n",
    "\n",
    "# Get argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = util.get_args(parser)\n",
    "print('blub')\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blub\n",
      "Namespace(Diters=5, batch_size=64, dataroot='./data/mnist', dataset='chairs', experiment=None, file='/home/yz6/.local/share/jupyter/runtime/kernel-501adab4-9060-412f-b239-c81f50efdb8d.json', gpu_device=0, image_size=64, lr=5e-05, max_iter=1000, nc=1, netD='', netG='', nz=10, workers=4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Using GPU device', 0L)\n",
      "('cnfg--------------------------------', 512)\n",
      "construced G_decoder\n",
      "('cnfg--------------------------------', 512)\n",
      "('netG:', NetG (\n",
      "  (decoder): Decoder (\n",
      "    (main): Sequential (\n",
      "      (initial.10-512.convt): ConvTranspose2d(10, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (initial.512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (initial.512.relu): ReLU (inplace)\n",
      "      (pyramid.512-256.convt): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.256.relu): ReLU (inplace)\n",
      "      (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.128.relu): ReLU (inplace)\n",
      "      (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.64.relu): ReLU (inplace)\n",
      "      (final.64-1.convt): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (final.1.tanh): Tanh ()\n",
      "    )\n",
      "  )\n",
      "))\n",
      "('netD:', NetD (\n",
      "  (encoder): Encoder (\n",
      "    (main): Sequential (\n",
      "      (initial.conv.1-64): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "      (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "      (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "      (pyramid.256-512.conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.512.relu): LeakyReLU (0.2, inplace)\n",
      "      (final.512-1.conv): Conv2d(512, 10, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder (\n",
      "    (main): Sequential (\n",
      "      (initial.10-512.convt): ConvTranspose2d(10, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (initial.512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (initial.512.relu): ReLU (inplace)\n",
      "      (pyramid.512-256.convt): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.256.relu): ReLU (inplace)\n",
      "      (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.128.relu): ReLU (inplace)\n",
      "      (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.64.relu): ReLU (inplace)\n",
      "      (final.64-1.convt): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (final.1.tanh): Tanh ()\n",
      "    )\n",
      "  )\n",
      "))\n",
      "('oneSide:', ONE_SIDED (\n",
      "  (main): ReLU ()\n",
      "))\n",
      "[  0/1000][101/10000] [    1] (0.20 m) MMD2_D 0.386083 hinge -0.000000 L2_AE_X 0.065771 L2_AE_Y 0.054880 loss_D -0.343853 Loss_G 0.633245 f_X 0.137802 f_Y -0.014532 |gD| 5.2429 |gG| 0.7789\n",
      "[  0/1000][202/10000] [    2] (0.37 m) MMD2_D 1.593841 hinge -0.000000 L2_AE_X 0.036219 L2_AE_Y 0.030225 loss_D 0.730927 Loss_G 1.281980 f_X 0.241620 f_Y -0.112799 |gD| 5.8414 |gG| 0.8001\n",
      "[  0/1000][303/10000] [    3] (0.54 m) MMD2_D 3.030426 hinge -0.000000 L2_AE_X 0.024453 L2_AE_Y 0.026682 loss_D 1.331733 Loss_G 1.752338 f_X 0.335116 f_Y -0.269713 |gD| 4.2230 |gG| 0.3889\n",
      "[  0/1000][404/10000] [    4] (0.71 m) MMD2_D 3.753547 hinge -0.000000 L2_AE_X 0.022451 L2_AE_Y 0.022123 loss_D 1.580817 Loss_G 1.939440 f_X 0.392565 f_Y -0.390575 |gD| 3.7957 |gG| 0.2004\n"
     ]
    }
   ],
   "source": [
    "if args.experiment is None:\n",
    "    args.experiment = 'samples'\n",
    "os.system('mkdir {0}'.format(args.experiment))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "    torch.cuda.set_device(args.gpu_device)\n",
    "    print(\"Using GPU device\", torch.cuda.current_device())\n",
    "else:\n",
    "    raise EnvironmentError(\"GPU device not available!\")\n",
    "\n",
    "args.manual_seed = 1126\n",
    "np.random.seed(seed=args.manual_seed)\n",
    "random.seed(args.manual_seed)\n",
    "torch.manual_seed(args.manual_seed)\n",
    "torch.cuda.manual_seed(args.manual_seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Get data\n",
    "data_length = 10000\n",
    "# trn_loader = util.load_data('/home/yz6/data/chairs/data.npy', args.batch_size, 64)\n",
    "# trn_dataset = util.get_data(args, train_flag=True)\n",
    "\n",
    "# trn_loader = torch.utils.data.DataLoader(trn_dataset,\n",
    "#                                          batch_size=args.batch_size,\n",
    "#                                          shuffle=True,\n",
    "#                                          num_workers=int(args.workers))\n",
    "\n",
    "# construct encoder/decoder modules\n",
    "hidden_dim = args.nz\n",
    "G_decoder = base_module.Decoder(args.image_size, args.nc, k=args.nz, ngf=64)\n",
    "print('construced G_decoder')\n",
    "D_encoder = base_module.Encoder(args.image_size, args.nc, k=hidden_dim, ndf=64)\n",
    "D_decoder = base_module.Decoder(args.image_size, args.nc, k=hidden_dim, ngf=64)\n",
    "\n",
    "netG = NetG(G_decoder)\n",
    "netD = NetD(D_encoder, D_decoder)\n",
    "one_sided = ONE_SIDED()\n",
    "print(\"netG:\", netG)\n",
    "print(\"netD:\", netD)\n",
    "print(\"oneSide:\", one_sided)\n",
    "\n",
    "netG.apply(base_module.weights_init)\n",
    "netD.apply(base_module.weights_init)\n",
    "one_sided.apply(base_module.weights_init)\n",
    "\n",
    "# sigma for MMD\n",
    "base = 1.0\n",
    "sigma_list = [1, 2, 4, 8, 16]\n",
    "sigma_list = [sigma / base for sigma in sigma_list]\n",
    "\n",
    "# put variable into cuda device\n",
    "fixed_noise = torch.cuda.FloatTensor(64, args.nz, 1, 1).normal_(0, 1)\n",
    "one = torch.cuda.FloatTensor([1])\n",
    "mone = one * -1\n",
    "if args.cuda:\n",
    "    netG.cuda()\n",
    "    netD.cuda()\n",
    "    one_sided.cuda()\n",
    "fixed_noise = Variable(fixed_noise, requires_grad=False)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerG = torch.optim.RMSprop(netG.parameters(), lr=args.lr)\n",
    "optimizerD = torch.optim.RMSprop(netD.parameters(), lr=args.lr)\n",
    "\n",
    "lambda_MMD = 1.0\n",
    "lambda_AE_X = 8.0\n",
    "lambda_AE_Y = 8.0\n",
    "lambda_rg = 16.0\n",
    "\n",
    "\n",
    "time = timeit.default_timer()\n",
    "gen_iterations = 0\n",
    "for t in range(args.max_iter):\n",
    "    data_iter = util.load_data('/home/yz6/data/chairs/data.npy', args.batch_size, 64)\n",
    "    i = 0\n",
    "    while (i < data_length):\n",
    "        # ---------------------------\n",
    "        #        Optimize over NetD\n",
    "        # ---------------------------\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "            Diters = 100\n",
    "            Giters = 1\n",
    "        else:\n",
    "            Diters = 5\n",
    "            Giters = 1\n",
    "\n",
    "        for j in range(Diters):\n",
    "            if i == data_length:\n",
    "                break\n",
    "\n",
    "            # clamp parameters of NetD encoder to a cube\n",
    "            # do not clamp paramters of NetD decoder!!!\n",
    "            for p in netD.encoder.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "            data = data_iter.next()\n",
    "            i += 1\n",
    "            netD.zero_grad()\n",
    "\n",
    "            #x_cpu, _ = data # torch.Floattensor\n",
    "            x_cpu =  torch.from_numpy(data)\n",
    "            \n",
    "            x = Variable(x_cpu.cuda())\n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            f_enc_X_D, f_dec_X_D = netD(x)\n",
    "\n",
    "            noise = torch.cuda.FloatTensor(batch_size, args.nz, 1, 1).normal_(0, 1)\n",
    "            noise = Variable(noise, volatile=True)  # total freeze netG\n",
    "            y = Variable(netG(noise).data)\n",
    "\n",
    "            f_enc_Y_D, f_dec_Y_D = netD(y)\n",
    "\n",
    "            # compute biased MMD2 and use ReLU to prevent negative value\n",
    "            mmd2_D = mix_rbf_mmd2(f_enc_X_D, f_enc_Y_D, sigma_list)\n",
    "            mmd2_D = F.relu(mmd2_D)\n",
    "\n",
    "            # compute rank hinge loss\n",
    "            #print('f_enc_X_D:', f_enc_X_D.size())\n",
    "            #print('f_enc_Y_D:', f_enc_Y_D.size())\n",
    "            one_side_errD = one_sided(f_enc_X_D.mean(0) - f_enc_Y_D.mean(0))\n",
    "\n",
    "            # compute L2-loss of AE\n",
    "            L2_AE_X_D = util.match(x.view(batch_size, -1), f_dec_X_D, 'L2')\n",
    "            L2_AE_Y_D = util.match(y.view(batch_size, -1), f_dec_Y_D, 'L2')\n",
    "\n",
    "            errD = torch.sqrt(mmd2_D) + lambda_rg * one_side_errD - lambda_AE_X * L2_AE_X_D - lambda_AE_Y * L2_AE_Y_D\n",
    "            errD.backward(mone)\n",
    "            optimizerD.step()\n",
    "\n",
    "        # ---------------------------\n",
    "        #        Optimize over NetG\n",
    "        # ---------------------------\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        for j in range(Giters):\n",
    "            if i == data_length:\n",
    "                break\n",
    "\n",
    "            data = data_iter.next()\n",
    "            i += 1\n",
    "            netG.zero_grad()\n",
    "\n",
    "            x_cpu =  torch.from_numpy(data)\n",
    "            x = Variable(x_cpu.cuda())\n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            f_enc_X, f_dec_X = netD(x)\n",
    "\n",
    "            noise = torch.cuda.FloatTensor(batch_size, args.nz, 1, 1).normal_(0, 1)\n",
    "            noise = Variable(noise)\n",
    "            y = netG(noise)\n",
    "\n",
    "            f_enc_Y, f_dec_Y = netD(y)\n",
    "\n",
    "            # compute biased MMD2 and use ReLU to prevent negative value\n",
    "            mmd2_G = mix_rbf_mmd2(f_enc_X, f_enc_Y, sigma_list)\n",
    "            mmd2_G = F.relu(mmd2_G)\n",
    "\n",
    "            # compute rank hinge loss\n",
    "            one_side_errG = one_sided(f_enc_X.mean(0) - f_enc_Y.mean(0))\n",
    "\n",
    "            errG = torch.sqrt(mmd2_G) + lambda_rg * one_side_errG\n",
    "            errG.backward(one)\n",
    "            optimizerG.step()\n",
    "\n",
    "            gen_iterations += 1\n",
    "\n",
    "        run_time = (timeit.default_timer() - time) / 60.0\n",
    "        print('[%3d/%3d][%3d/%3d] [%5d] (%.2f m) MMD2_D %.6f hinge %.6f L2_AE_X %.6f L2_AE_Y %.6f loss_D %.6f Loss_G %.6f f_X %.6f f_Y %.6f |gD| %.4f |gG| %.4f'\n",
    "              % (t, args.max_iter, i, data_length, gen_iterations, run_time,\n",
    "                 mmd2_D.data[0], one_side_errD.data[0],\n",
    "                 L2_AE_X_D.data[0], L2_AE_Y_D.data[0],\n",
    "                 errD.data[0], errG.data[0],\n",
    "                 f_enc_X_D.mean().data[0], f_enc_Y_D.mean().data[0],\n",
    "                 base_module.grad_norm(netD), base_module.grad_norm(netG)))\n",
    "\n",
    "        if gen_iterations % 500 == 0:\n",
    "            y_fixed = netG(fixed_noise)\n",
    "            y_fixed.data = y_fixed.data.mul(0.5).add(0.5)\n",
    "            f_dec_X_D = f_dec_X_D.view(f_dec_X_D.size(0), args.nc, args.image_size, args.image_size)\n",
    "            f_dec_X_D.data = f_dec_X_D.data.mul(0.5).add(0.5)\n",
    "            vutils.save_image(y_fixed.data, '{0}/fake_samples_{1}.png'.format(args.experiment, gen_iterations))\n",
    "            vutils.save_image(f_dec_X_D.data, '{0}/decode_samples_{1}.png'.format(args.experiment, gen_iterations))\n",
    "\n",
    "    if t % 50 == 0:\n",
    "        torch.save(netG.state_dict(), '{0}/netG_iter_{1}.pth'.format(args.experiment, t))\n",
    "        torch.save(netD.state_dict(), '{0}/netD_iter_{1}.pth'.format(args.experiment, t))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
