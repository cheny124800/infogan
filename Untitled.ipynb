{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(Diters=5, batch_size=64, dataroot='./data/mnist', dataset='chairs', experiment=None, file='/home/yz6/.local/share/jupyter/runtime/kernel-c8cd92bd-9042-4b2b-808e-a442ccd4feee.json', gpu_device=0, image_size=64, lr=5e-05, max_iter=1000, nc=1, netD='', netG='', nz=10, workers=4)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import timeit\n",
    "#from Exp import Exp\n",
    "\n",
    "import util\n",
    "import numpy as np\n",
    "\n",
    "import base_module\n",
    "from mmd import mix_rbf_mmd2\n",
    "\n",
    "\n",
    "# NetG is a decoder\n",
    "# input: batch_size * nz * 1 * 1\n",
    "# output: batch_size * nc * image_size * image_size\n",
    "class NetG(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super(NetG, self).__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.decoder(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "# NetD is an encoder + decoder\n",
    "# input: batch_size * nc * image_size * image_size\n",
    "# f_enc_X: batch_size * k * 1 * 1\n",
    "# f_dec_X: batch_size * nc * image_size * image_size\n",
    "class NetD(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(NetD, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.Q = nn.Sigmoid()\n",
    "        self.D1 = nn.LogSoftmax()\n",
    "        self.D2 = nn.LogSoftmax()\n",
    "        self.D3 = nn.LogSoftmax()\n",
    "        self.C_std = torch.exp\n",
    "        #self.decoder = decoder\n",
    "\n",
    "    def forward(self, input):\n",
    "        f_enc_X = self.encoder(input)\n",
    "        f_enc_X = f_enc_X.view(input.size(0), -1)\n",
    "        q = self.Q(f_enc_X[:, 0])\n",
    "        d1 = self.D1(f_enc_X[:, 1:21])\n",
    "        d2 = self.D2(f_enc_X[:, 21:41])\n",
    "        d3 = self.D3(f_enc_X[:, 41:61])\n",
    "        c_std = self.C_std(f_enc_X[:, 61])\n",
    "        mu = f_enc_X[:, 62]\n",
    "        return f_enc_X, q, d1, d2, d3, mu, c_std\n",
    "\n",
    "\n",
    "class ONE_SIDED(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ONE_SIDED, self).__init__()\n",
    "\n",
    "        main = nn.ReLU()\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(-input)\n",
    "        output = -output.mean()\n",
    "        return output\n",
    "\n",
    "\n",
    "    \n",
    "# Get argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = util.get_args(parser)\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_multinomial(num_classes, size):\n",
    "    c = np.random.multinomial(1, num_classes*[1./num_classes], size=size)\n",
    "    return c\n",
    "\n",
    "def sample_uniform(size):\n",
    "    c = np.random.uniform(0, 1, size=size)\n",
    "    return c\n",
    "def sample_random(size):\n",
    "    return np.random.randn(*size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Using GPU device', 0L)\n",
      "('cnfg--------------------------------', 512)\n",
      "('netG:', NetG (\n",
      "  (decoder): Decoder (\n",
      "    (main): Sequential (\n",
      "      (initial.189-512.convt): ConvTranspose2d(189, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (initial.512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (initial.512.relu): ReLU (inplace)\n",
      "      (pyramid.512-256.convt): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.256.relu): ReLU (inplace)\n",
      "      (pyramid.256-128.convt): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.128.relu): ReLU (inplace)\n",
      "      (pyramid.128-64.convt): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.64.relu): ReLU (inplace)\n",
      "      (final.64-1.convt): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (final.1.tanh): Tanh ()\n",
      "    )\n",
      "  )\n",
      "))\n",
      "('netD:', NetD (\n",
      "  (encoder): Encoder (\n",
      "    (main): Sequential (\n",
      "      (initial.conv.1-64): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (initial.relu.64): LeakyReLU (0.2, inplace)\n",
      "      (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
      "      (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
      "      (pyramid.256-512.conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (pyramid.512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (pyramid.512.relu): LeakyReLU (0.2, inplace)\n",
      "      (final.512-1.conv): Conv2d(512, 63, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (Q): Sigmoid ()\n",
      "  (D1): LogSoftmax ()\n",
      "  (D2): LogSoftmax ()\n",
      "  (D3): LogSoftmax ()\n",
      "))\n",
      "ConvTranspose2d\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "BatchNorm2d\n",
      "ReLU\n",
      "ReLU\n",
      "ConvTranspose2d\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "BatchNorm2d\n",
      "ReLU\n",
      "ReLU\n",
      "ConvTranspose2d\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "BatchNorm2d\n",
      "ReLU\n",
      "ReLU\n",
      "ConvTranspose2d\n",
      "ConvTranspose2d\n",
      "BatchNorm2d\n",
      "BatchNorm2d\n",
      "ReLU\n",
      "ReLU\n",
      "ConvTranspose2d\n",
      "ConvTranspose2d\n",
      "Tanh\n",
      "Tanh\n",
      "Sequential\n",
      "Sequential\n",
      "Decoder\n",
      "Decoder\n",
      "NetG\n",
      "NetG\n",
      "Conv2d\n",
      "Conv2d\n",
      "LeakyReLU\n",
      "LeakyReLU\n",
      "Conv2d\n",
      "Conv2d\n",
      "BatchNorm2d\n",
      "BatchNorm2d\n",
      "LeakyReLU\n",
      "LeakyReLU\n",
      "Conv2d\n",
      "Conv2d\n",
      "BatchNorm2d\n",
      "BatchNorm2d\n",
      "LeakyReLU\n",
      "LeakyReLU\n",
      "Conv2d\n",
      "Conv2d\n",
      "BatchNorm2d\n",
      "BatchNorm2d\n",
      "LeakyReLU\n",
      "LeakyReLU\n",
      "Conv2d\n",
      "Conv2d\n",
      "Sequential\n",
      "Sequential\n",
      "Encoder\n",
      "Encoder\n",
      "Sigmoid\n",
      "Sigmoid\n",
      "LogSoftmax\n",
      "LogSoftmax\n",
      "LogSoftmax\n",
      "LogSoftmax\n",
      "LogSoftmax\n",
      "LogSoftmax\n",
      "NetD\n",
      "NetD\n",
      "shape --------\n",
      "Variable containing:\n",
      " 0.4996\n",
      " 0.4997\n",
      " 0.5000\n",
      " 0.5001\n",
      " 0.4999\n",
      " 0.4999\n",
      " 0.4998\n",
      " 0.4997\n",
      " 0.4998\n",
      " 0.4998\n",
      " 0.4998\n",
      " 0.5001\n",
      " 0.4996\n",
      " 0.5003\n",
      " 0.4995\n",
      " 0.5001\n",
      " 0.4997\n",
      " 0.5002\n",
      " 0.4998\n",
      " 0.4999\n",
      " 0.4996\n",
      " 0.4997\n",
      " 0.4995\n",
      " 0.4999\n",
      " 0.4992\n",
      " 0.4999\n",
      " 0.5000\n",
      " 0.4998\n",
      " 0.4996\n",
      " 0.4999\n",
      " 0.5000\n",
      " 0.5003\n",
      " 0.4999\n",
      " 0.5000\n",
      " 0.4999\n",
      " 0.4999\n",
      " 0.5002\n",
      " 0.4997\n",
      " 0.4996\n",
      " 0.4996\n",
      " 0.4996\n",
      " 0.4996\n",
      " 0.4999\n",
      " 0.4997\n",
      " 0.4997\n",
      " 0.4997\n",
      " 0.4998\n",
      " 0.4998\n",
      " 0.4999\n",
      " 0.4999\n",
      " 0.4998\n",
      " 0.4998\n",
      " 0.4996\n",
      " 0.4997\n",
      " 0.4999\n",
      " 0.5003\n",
      " 0.4999\n",
      " 0.4997\n",
      " 0.4997\n",
      " 0.4999\n",
      " 0.4996\n",
      " 0.4999\n",
      " 0.5003\n",
      " 0.5008\n",
      "[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "torch.Size([64, 20])\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  0.1572\n",
      " -0.5721\n",
      " -0.0706\n",
      " -0.0274\n",
      "  0.5218\n",
      "  0.0727\n",
      " -1.6495\n",
      " -1.3412\n",
      "  1.5245\n",
      "  1.1557\n",
      " -2.9585\n",
      " -1.2160\n",
      " -2.3561\n",
      " -0.7722\n",
      "  0.7005\n",
      " -1.3037\n",
      " -0.2157\n",
      "  1.8351\n",
      " -0.7990\n",
      " -1.9982\n",
      "  2.1843\n",
      " -1.2349\n",
      " -0.5246\n",
      "  0.9018\n",
      " -1.2123\n",
      "  0.4146\n",
      "  0.0542\n",
      "  0.5442\n",
      "  1.3499\n",
      "  0.1594\n",
      "  1.0844\n",
      " -0.8879\n",
      "  0.5459\n",
      " -0.4466\n",
      " -0.2854\n",
      " -0.8852\n",
      " -0.7624\n",
      "  0.4046\n",
      " -0.1154\n",
      " -1.2400\n",
      "  1.0512\n",
      "  1.3225\n",
      "  0.7173\n",
      "  0.1801\n",
      " -0.3770\n",
      " -1.2262\n",
      "  0.0541\n",
      " -2.8387\n",
      " -0.1204\n",
      " -2.3854\n",
      " -0.2739\n",
      "  0.1514\n",
      "  1.0810\n",
      " -0.2042\n",
      "  0.8556\n",
      " -3.2762\n",
      " -0.0252\n",
      "  2.1745\n",
      "  0.5504\n",
      "  0.5167\n",
      " -1.5180\n",
      " -0.0343\n",
      " -0.8309\n",
      " -0.0923\n",
      "[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.9995\n",
      " 1.0003\n",
      " 0.9991\n",
      " 0.9970\n",
      " 0.9999\n",
      " 1.0001\n",
      " 1.0001\n",
      " 0.9991\n",
      " 0.9969\n",
      " 0.9991\n",
      " 0.9982\n",
      " 1.0015\n",
      " 0.9976\n",
      " 0.9989\n",
      " 1.0000\n",
      " 1.0004\n",
      " 0.9999\n",
      " 0.9987\n",
      " 0.9979\n",
      " 0.9986\n",
      " 0.9986\n",
      " 0.9994\n",
      " 0.9995\n",
      " 0.9985\n",
      " 0.9996\n",
      " 0.9996\n",
      " 1.0000\n",
      " 0.9985\n",
      " 1.0002\n",
      " 1.0005\n",
      " 1.0000\n",
      " 1.0010\n",
      " 0.9991\n",
      " 0.9980\n",
      " 0.9975\n",
      " 0.9972\n",
      " 1.0001\n",
      " 0.9974\n",
      " 0.9980\n",
      " 0.9990\n",
      " 0.9989\n",
      " 1.0000\n",
      " 1.0002\n",
      " 0.9997\n",
      " 0.9985\n",
      " 0.9984\n",
      " 0.9990\n",
      " 0.9986\n",
      " 0.9998\n",
      " 0.9974\n",
      " 0.9990\n",
      " 0.9979\n",
      " 0.9985\n",
      " 0.9986\n",
      " 0.9989\n",
      " 1.0011\n",
      " 1.0001\n",
      " 0.9995\n",
      " 0.9990\n",
      " 0.9982\n",
      " 0.9984\n",
      " 0.9999\n",
      " 0.9978\n",
      " 0.9997\n",
      "[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 2.1022\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "error\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of variables tuple is volatile",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-027f195595c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0merrD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0moptimizerD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of variables tuple is volatile"
     ]
    }
   ],
   "source": [
    "if args.experiment is None:\n",
    "    args.experiment = 'samples'\n",
    "os.system('mkdir {0}'.format(args.experiment))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "    torch.cuda.set_device(args.gpu_device)\n",
    "    print(\"Using GPU device\", torch.cuda.current_device())\n",
    "else:\n",
    "    raise EnvironmentError(\"GPU device not available!\")\n",
    "\n",
    "BATCH_SIZE = 64 # args.batch_size\n",
    "IMG_SIZE = args.image_size # 64\n",
    "NC = 1 # number of input channels\n",
    "Z_dim = 128\n",
    "d1_dim = 20\n",
    "c_dim = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args.manual_seed = 1126\n",
    "np.random.seed(seed=args.manual_seed)\n",
    "random.seed(args.manual_seed)\n",
    "torch.manual_seed(args.manual_seed)\n",
    "torch.cuda.manual_seed(args.manual_seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Get data\n",
    "data_length = 10000\n",
    "# trn_loader = util.load_data('/home/yz6/data/chairs/data.npy', args.batch_size, 64)\n",
    "# trn_dataset = util.get_data(args, train_flag=True)\n",
    "\n",
    "# trn_loader = torch.utils.data.DataLoader(trn_dataset,\n",
    "#                                          batch_size=args.batch_size,\n",
    "#                                          shuffle=True,\n",
    "#                                          num_workers=int(args.workers))\n",
    "\n",
    "# construct encoder/decoder modules\n",
    "G_decoder = base_module.Decoder(IMG_SIZE, NC, k=189, ngf=64)\n",
    "D_encoder = base_module.Encoder(IMG_SIZE, NC, k=63, ndf=64)\n",
    "#D_decoder = base_module.Decoder(args.image_size, args.nc, k=hidden_dim, ngf=64)\n",
    "\n",
    "netG = NetG(G_decoder)\n",
    "netD = NetD(D_encoder)\n",
    "#one_sided = ONE_SIDED()\n",
    "print(\"netG:\", netG)\n",
    "print(\"netD:\", netD)\n",
    "#print(\"oneSide:\", one_sided)\n",
    "\n",
    "netG.apply(base_module.weights_init)\n",
    "netD.apply(base_module.weights_init)\n",
    "#one_sided.apply(base_module.weights_init)\n",
    "\n",
    "# sigma for MMD\n",
    "# base = 1.0\n",
    "# sigma_list = [1, 2, 4, 8, 16]\n",
    "# sigma_list = [sigma / base for sigma in sigma_list]\n",
    "\n",
    "# put variable into cuda device\n",
    "#fixed_noise = torch.cuda.FloatTensor(64, args.nz, 1, 1).normal_(0, 1)\n",
    "def create_noise_cuda():\n",
    "    noise_np_list = list()\n",
    "    noise_np_list.append(sample_random([BATCH_SIZE, 128]))\n",
    "    noise_np_list.append(sample_multinomial(num_classes=20, size=BATCH_SIZE))\n",
    "    noise_np_list.append(sample_multinomial(num_classes=20, size=BATCH_SIZE))\n",
    "    noise_np_list.append(sample_multinomial(num_classes=20, size=BATCH_SIZE))\n",
    "    noise_np_list.append(sample_uniform(size=(BATCH_SIZE, 1)))\n",
    "\n",
    "    noise_vector = np.concatenate(noise_np_list, axis=1)\n",
    "    noise_vector = np.expand_dims(noise_vector, axis=2)\n",
    "    noise_vector = np.expand_dims(noise_vector, axis=3)\n",
    "    \n",
    "    \n",
    "    noise = torch.from_numpy(noise_vector.astype('float32')).cuda()\n",
    "    return noise\n",
    "\n",
    "    \n",
    "one = torch.cuda.FloatTensor([1])\n",
    "mone = one * -1\n",
    "if args.cuda:\n",
    "    netG.cuda()\n",
    "    netD.cuda()\n",
    "    #one_sided.cuda()\n",
    "\n",
    "# setup optimizer\n",
    "optimizerG = torch.optim.RMSprop(netG.parameters(), lr=args.lr)\n",
    "optimizerD = torch.optim.RMSprop(netD.parameters(), lr=args.lr)\n",
    "\n",
    "lambda_MMD = 1.0\n",
    "lambda_AE_X = 8.0\n",
    "lambda_AE_Y = 8.0\n",
    "lambda_rg = 16.0\n",
    "\n",
    "\n",
    "time = timeit.default_timer()\n",
    "gen_iterations = 0\n",
    "for t in range(args.max_iter):\n",
    "    data_iter = util.load_data('/home/yz6/data/chairs/data.npy', BATCH_SIZE, IMG_SIZE)\n",
    "    i = 0\n",
    "    while (i < data_length):\n",
    "        # ---------------------------\n",
    "        #        Optimize over NetD\n",
    "        # ---------------------------\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "            Diters = 100\n",
    "            Giters = 1\n",
    "        else:\n",
    "            Diters = 5\n",
    "            Giters = 1\n",
    "\n",
    "        for j in range(Diters):\n",
    "            if i == data_length:\n",
    "                break\n",
    "\n",
    "            # clamp parameters of NetD encoder to a cube\n",
    "            # do not clamp paramters of NetD decoder!!!\n",
    "            for p in netD.encoder.parameters():\n",
    "                p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "            data = data_iter.next()\n",
    "            i += 1\n",
    "            netD.zero_grad()\n",
    "\n",
    "            #x_cpu, _ = data # torch.Floattensor\n",
    "            x_cpu =  torch.from_numpy(data)\n",
    "            \n",
    "            x = Variable(x_cpu.cuda())\n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            f_enc_X_D, q_real, _, _, _, _, _ = netD(x)\n",
    "            print('shape --------')\n",
    "            print(q)\n",
    "            print(d1.size())\n",
    "            print(d2.size())\n",
    "            print(d3.size())\n",
    "            print(mu)\n",
    "            print(c_std)\n",
    "\n",
    "            noise = create_noise_cuda()\n",
    "            noise = Variable(noise, volatile=True)  # total freeze netG\n",
    "            y = Variable(netG(noise).data)\n",
    "\n",
    "            f_enc_Y_D, q_fake, d1, d2, d3, mu, c_std = netD(y)\n",
    "\n",
    "            # compute biased MMD2 and use ReLU to prevent negative value\n",
    "#             mmd2_D = mix_rbf_mmd2(f_enc_X_D, f_enc_Y_D, sigma_list)\n",
    "#             mmd2_D = F.relu(mmd2_D)\n",
    "\n",
    "            # compute rank hinge loss\n",
    "            #print('f_enc_X_D:', f_enc_X_D.size())\n",
    "            #print('f_enc_Y_D:', f_enc_Y_D.size())\n",
    "            #one_side_errD = one_sided(f_enc_X_D.mean(0) - f_enc_Y_D.mean(0))\n",
    "\n",
    "            # compute L2-loss of AE\n",
    "            #L2_AE_X_D = util.match(x.view(batch_size, -1), f_dec_X_D, 'L2')\n",
    "            #L2_AE_Y_D = util.match(y.view(batch_size, -1), f_dec_Y_D, 'L2')\n",
    "            D_loss = torch.mean(torch.log(q_real + 1e-8) + torch.log(1 - q_fake + 1e-8))\n",
    "            \n",
    "            crossent_1 = torch.mean(torch.sum(noise[:, 128:148] * d1, dim=1))\n",
    "            crossent_2 = torch.mean(torch.sum(noise[:, 148:168] * d2, dim=1))\n",
    "            crossent_3 = torch.mean(torch.sum(noise[:, 168:188] * d3, dim=1))\n",
    "            std_contig = torch.sqrt(c_std)\n",
    "            epsilon = (noise[:, 188] - mu) / (std_contig + 1e-8)\n",
    "            ll_continuous = torch.sum( - torch.log(std_contig + 1e-8) - 0.5 * epsilon * epsilon, dim=1)\n",
    "            \n",
    "            #ent_loss = torch.mean(torch.sum(c * torch.log(c + 1e-8), dim=1))\n",
    "\n",
    "            lamdba_c = 0.05\n",
    "            lamdba_d = 1\n",
    "            mi_loss = lamdba_d * (crossent_1 + crossent_2 + crossent_3) + lamdba_c * ll_continuous\n",
    "\n",
    "            #errD = torch.sqrt(mmd2_D) + lambda_rg * one_side_errD - lambda_AE_X * L2_AE_X_D - lambda_AE_Y * L2_AE_Y_D\n",
    "            errD = D_loss - mi_loss\n",
    "            print(errG)\n",
    "            print('error')\n",
    "            print(j)\n",
    "            errD.backward(mone)\n",
    "            optimizerD.step()\n",
    "\n",
    "        # ---------------------------\n",
    "        #        Optimize over NetG\n",
    "        # ---------------------------\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        for j in range(Giters):\n",
    "            if i == data_length:\n",
    "                break\n",
    "\n",
    "            data = data_iter.next()\n",
    "            i += 1\n",
    "            netG.zero_grad()\n",
    "\n",
    "            x_cpu =  torch.from_numpy(data)\n",
    "            x = Variable(x_cpu.cuda())\n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            f_enc_X, f_dec_X = netD(x)\n",
    "\n",
    "            noise = torch.cuda.FloatTensor(batch_size, args.nz, 1, 1).normal_(0, 1)\n",
    "            noise = Variable(noise)\n",
    "            y = netG(noise)\n",
    "\n",
    "            f_enc_Y, f_dec_Y = netD(y)\n",
    "\n",
    "            # compute biased MMD2 and use ReLU to prevent negative value\n",
    "            mmd2_G = mix_rbf_mmd2(f_enc_X, f_enc_Y, sigma_list)\n",
    "            mmd2_G = F.relu(mmd2_G)\n",
    "\n",
    "            # compute rank hinge loss\n",
    "            one_side_errG = one_sided(f_enc_X.mean(0) - f_enc_Y.mean(0))\n",
    "\n",
    "            errG = torch.sqrt(mmd2_G) + lambda_rg * one_side_errG\n",
    "\n",
    "            errG.backward(one)\n",
    "            optimizerG.step()\n",
    "\n",
    "            gen_iterations += 1\n",
    "\n",
    "        run_time = (timeit.default_timer() - time) / 60.0\n",
    "        print('[%3d/%3d][%3d/%3d] [%5d] (%.2f m) MMD2_D %.6f hinge %.6f L2_AE_X %.6f L2_AE_Y %.6f loss_D %.6f Loss_G %.6f f_X %.6f f_Y %.6f |gD| %.4f |gG| %.4f'\n",
    "              % (t, args.max_iter, i, data_length, gen_iterations, run_time,\n",
    "                 mmd2_D.data[0], one_side_errD.data[0],\n",
    "                 L2_AE_X_D.data[0], L2_AE_Y_D.data[0],\n",
    "                 errD.data[0], errG.data[0],\n",
    "                 f_enc_X_D.mean().data[0], f_enc_Y_D.mean().data[0],\n",
    "                 base_module.grad_norm(netD), base_module.grad_norm(netG)))\n",
    "\n",
    "        if gen_iterations % 500 == 0:\n",
    "            y_fixed = netG(fixed_noise)\n",
    "            y_fixed.data = y_fixed.data.mul(0.5).add(0.5)\n",
    "            f_dec_X_D = f_dec_X_D.view(f_dec_X_D.size(0), args.nc, args.image_size, args.image_size)\n",
    "            f_dec_X_D.data = f_dec_X_D.data.mul(0.5).add(0.5)\n",
    "            vutils.save_image(y_fixed.data, '{0}/fake_samples_{1}.png'.format(args.experiment, gen_iterations))\n",
    "            vutils.save_image(f_dec_X_D.data, '{0}/decode_samples_{1}.png'.format(args.experiment, gen_iterations))\n",
    "\n",
    "    if t % 50 == 0:\n",
    "        torch.save(netG.state_dict(), '{0}/netG_iter_{1}.pth'.format(args.experiment, t))\n",
    "        torch.save(netD.state_dict(), '{0}/netD_iter_{1}.pth'.format(args.experiment, t))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
